---
layout:     post
title:      线程中的日志管理
subtitle:   在线程中通过统一的uuid标识每次日志并且实现序列号排序
date:       2018-10-25
author:     uncledrew
header-img: img/post-bg-re-vs-ng2.jpg
catalog: true
tags:
    - SpringBoot
    - Logger
    - ThreadLocal
    - 日志管理
    - 弱引用
    - 内存泄露
---

> 纸上得来终觉浅，绝知此事要躬行。
>
> [我的博客](http://uncledrew.405go.cn/)


# 前言
前端向后端发起一次请求，在一个线程中进行多次 log 的时候，如果并发量不大，在控制台或者日志文件中可以方便的找出本次请求的所有日志。
但如果是一个高并发的系统，会造成日志顺序的混乱，对开发人员排查问题极其不便。

# 解决方案
1. 进入线程时，往 ThreadLocal 中存入 uuid 和 sequence(初始为0)。

2. 重写 org.slf4j.Logger，封装自己的 MxLogger。

3. 每次调用 info()或 debug()或 error()方法时，取出 ThreadLocal 中的 uuid 和 sequence，一起记录到日志中。

4. 将 sequence 加1，重新放到 ThreadLocal中。

5. 重新调用 info()或 debug()或 error()方法时，循环3和4的步骤。

6. 线程结束时，销毁 ThreadLocal。

# 具体实现
引入需要的依赖：

```
<!-- fasterxml 支持 -->
<dependency>
    <groupId>com.fasterxml.jackson.core</groupId>
    <artifactId>jackson-core</artifactId>
    <version>2.8.3</version>
</dependency>
<dependency>
    <groupId>com.fasterxml.jackson.core</groupId>
    <artifactId>jackson-databind</artifactId>
    <version>2.8.3</version>
</dependency>

<!-- logback 支持 -->
<dependency>
    <groupId>org.slf4j</groupId>
    <artifactId>jcl-over-slf4j</artifactId>
    <version>1.7.7</version>
</dependency>
<dependency>
    <groupId>ch.qos.logback</groupId>
    <artifactId>logback-classic</artifactId>
    <version>1.1.2</version>
</dependency>
<dependency>
    <groupId>net.logstash.logback</groupId>
    <artifactId>logstash-logback-encoder</artifactId>
    <version>4.8</version>
</dependency>
```

添加 ObjectMapper 的单例：

```
import com.fasterxml.jackson.databind.ObjectMapper;

public class ObjectMapperFactory {

    private ObjectMapperFactory() {
    }

    private static class ObjectMapperInstance {
        private static final ObjectMapper INSTANCE = new ObjectMapper();
    }

    public static ObjectMapper getInstance() {
        return ObjectMapperInstance.INSTANCE;
    }

}
```

添加一个 MxLogProperty 类：

```
public class MxLogProperty {

    public static final String KEY_UUID = "uuid";

    public static final String KEY_SEQUENCE = "sequence";

    public static final String KEY_MESSAGE = "log_content";

    public static final String KEY_EXCEPTION_INFO = "exception_info";

}
```

添加重新封装 org.slf4j.Logger 的 MxLogger 类：

```
import com.fasterxml.jackson.core.JsonProcessingException;
import org.slf4j.Logger;

import java.util.HashMap;
import java.util.Map;

public class MxLogger {

    private Logger logger;

    public MxLogger(Logger logger) {
        this.logger = logger;
    }

    private MxLogger() {
    }

    public void info(String message) {
        Map<String, Object> logMap = new HashMap<>();
        logMap.put(MxLogProperty.KEY_UUID, MxLoggerThreadLocal.getUUID());
        logMap.put(MxLogProperty.KEY_SEQUENCE, MxLoggerThreadLocal.getLogSequence());
        logMap.put(MxLogProperty.KEY_MESSAGE, message);
        logger.info(writeValueAsString(logMap));
    }

    private String writeValueAsString(Object obj) {
        try {
            return ObjectMapperFactory.getInstance().writeValueAsString(obj);
        } catch (JsonProcessingException e) {
            e.printStackTrace();
            throw new RuntimeException("ObjectMapper将对象转换字符串时发生异常，异常信息是[" + e.getMessage() + "]");
        }
    }
}
```

添加 MxLogger 的工厂类 MxLoggerFactory：
```
import org.slf4j.LoggerFactory;

public class MxLoggerFactory {

    public static MxLogger getLogger(Class<?> clazz) {
        MxLogger mxLogger = new MxLogger(LoggerFactory.getLogger(clazz));
        return mxLogger;
    }
}
```

添加 MxLoggerThreadLocal 类，用于存放 uuid 和 sequence：
```
public class MxLoggerThreadLocal {

    private static ThreadLocal<Integer> logSequenceThreadLocal = new ThreadLocal<>();
    private static ThreadLocal<String> uuidThreadLocal = new ThreadLocal<>();

    public static Integer getLogSequence() {
        Integer sequence = logSequenceThreadLocal.get();
        if (sequence == null) {
            sequence = 0;
            logSequenceThreadLocal.set(sequence);
        }
        increaseLogSequence();
        return sequence;
    }

    public static void increaseLogSequence() {
        Integer sequence = logSequenceThreadLocal.get();
        int increasedSequence = sequence.intValue() + 1;
        logSequenceThreadLocal.set(increasedSequence);
    }

    public static String getUUID() {
        return uuidThreadLocal.get();
    }

    public static void setUUID(String uuid) {
        uuidThreadLocal.set(uuid);
    }

    public static void remove() {
        uuidThreadLocal.remove();
        logSequenceThreadLocal.remove();
    }
}
```

# 应用实战
添加测试类：

```
import org.junit.Test;

public class MxLoggerTest {

    private static final MxLogger logger = MxLoggerFactory.getLogger(MxLoggerTest.class);

    @Test
    public void testLogger() {
        try {
            MxLoggerThreadLocal.setUUID("123456");
            logger.info("log start");
            doLog();
        } finally {
            logger.info("log end");
            MxLoggerThreadLocal.remove();
        }
    }

    private void doLog() {
        logger.info("第一条日志");
        logger.info("第二条日志");
        logger.info("第三条日志");
    }
}
```

查看控制台结果：

```
18:17:18.645 [main] INFO com.uncledrew.MxLoggerTest - {"sequence":0,"log_content":"log start","uuid":"123456"}
18:17:18.654 [main] INFO com.uncledrew.MxLoggerTest - {"sequence":1,"log_content":"第一条日志","uuid":"123456"}
18:17:18.654 [main] INFO com.uncledrew.MxLoggerTest - {"sequence":2,"log_content":"第二条日志","uuid":"123456"}
18:17:18.654 [main] INFO com.uncledrew.MxLoggerTest - {"sequence":3,"log_content":"第三条日志","uuid":"123456"}
18:17:18.654 [main] INFO com.uncledrew.MxLoggerTest - {"sequence":4,"log_content":"log end","uuid":"123456"}

```

# 扩展
可以利用 Kafka 的消息收发机制，主程序将日志包装好作为消息体，然后通过 Elasticsearch 读取消息并且存储起来。
最后在 Kibana 图形界面进行搜索、分析和可视化存储在 Elasticsearch 指标中的日志数据。

# ELK 相关概念介绍
ELK 是 Elasticsearch、Logstash、Kibana 这三个核心套件的简称。

- Elasticsearch 是实时全文搜索和分析引擎，提供搜集、分析、存储数据三大功能；是一套开放 REST 和 JAVA API 等结构提供高效搜索功能，可扩展的分布式系统。它构建于 Apache Lucene 搜索引擎库之上。
- Logstash 是一个用来搜集、分析、过滤日志的工具。它支持几乎任何类型的日志，包括系统日志、错误日志和自定义应用程序日志。它可以从许多来源接收日志，这些来源包括 syslog、消息传递（例如 RabbitMQ）和JMX，它能够以多种方式输出数据，包括电子邮件、websockets 和 Elasticsearch。
- Kibana 是一个基于 Web 的图形界面，用于搜索、分析和可视化存储在 Elasticsearch 指标中的日志数据。它利用 Elasticsearch 的 REST 接口来检索数据，不仅允许用户创建他们自己的数据的定制仪表板视图，还允许他们以特殊的方式查询和过滤数据

# 参考
[快速搭建ELK日志分析系统](https://www.cnblogs.com/yuhuLin/p/7018858.html)

[随笔分类 - EleasticSearch](https://www.cnblogs.com/Wolfmanlq/category/897055.html)

[深入分析 ThreadLocal 内存泄漏问题](http://www.importnew.com/22039.html)

[Kafka快速入门](http://colobu.com/2014/08/06/kafka-quickstart/)

[Kafka 0.9.0 Documentation](http://kafka.apache.org/090/documentation.html#quickstart)